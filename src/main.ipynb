{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "emdat_path = \"/home/nissim/Documents/dev/arg-inundaciones/data/public_emdat_custom_request_2025-06-26_fe5041a7-dc26-4391-aef0-f49e5e5d8657.xlsx\"\n",
    "emdat = pd.read_excel(emdat_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column dtype: object\n",
      "Column type: <class 'pandas.core.series.Series'>\n",
      "\n",
      "First element type: <class 'str'>\n",
      "Second element type: <class 'str'>\n",
      "\n",
      "First row content:\n",
      "[{\"adm1_code\":431,\"adm1_name\":\"Catamarca\"},{\"adm1_code\":434,\"adm1_name\":\"Cordoba\"},{\"adm1_code\":438,\"adm1_name\":\"Jujuy\"},{\"adm1_code\":440,\"adm1_name\":\"La Rioja\"},{\"adm1_code\":445,\"adm1_name\":\"Salta\"},{\"adm1_code\":450,\"adm1_name\":\"Santiago Del Estero\"},{\"adm1_code\":452,\"adm1_name\":\"Tucuman\"}]\n",
      "\n",
      "Second row content:\n",
      "[{\"adm1_code\":430,\"adm1_name\":\"Buenos Aires D.f.\"},{\"adm1_code\":434,\"adm1_name\":\"Cordoba\"},{\"adm1_code\":439,\"adm1_name\":\"La Pampa\"},{\"adm2_code\":4386,\"adm2_name\":\"Avellaneda\"},{\"adm2_code\":4395,\"adm2_name\":\"Berisso\"},{\"adm2_code\":4445,\"adm2_name\":\"Lanus\"},{\"adm2_code\":4477,\"adm2_name\":\"Quilmes\"},{\"adm2_code\":82738,\"adm2_name\":\"San Miguel\"},{\"adm2_code\":190525,\"adm2_name\":\"San  Fernando\"},{\"adm2_code\":4631,\"adm2_name\":\"Parana\"},{\"adm2_code\":4836,\"adm2_name\":\"Rosario\"}]\n",
      "\n",
      "It's a string - might need JSON parsing\n",
      "Successfully parsed as JSON: <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# Check the data type of the column\n",
    "print(\"Column dtype:\", emdat[\"Admin Units\"].dtype)\n",
    "print(\"Column type:\", type(emdat[\"Admin Units\"]))\n",
    "\n",
    "# Check the type of individual elements\n",
    "print(\"\\nFirst element type:\", type(emdat[\"Admin Units\"].iloc[0]))\n",
    "print(\"Second element type:\", type(emdat[\"Admin Units\"].iloc[1]))\n",
    "\n",
    "# Look at the actual content of a few rows\n",
    "print(\"\\nFirst row content:\")\n",
    "print(emdat[\"Admin Units\"].iloc[0])\n",
    "print(\"\\nSecond row content:\")\n",
    "print(emdat[\"Admin Units\"].iloc[1])\n",
    "\n",
    "# Check if it's a string that needs parsing\n",
    "if isinstance(emdat[\"Admin Units\"].iloc[0], str):\n",
    "    print(\"\\nIt's a string - might need JSON parsing\")\n",
    "    import json\n",
    "\n",
    "    try:\n",
    "        parsed = json.loads(emdat[\"Admin Units\"].iloc[0])\n",
    "        print(\"Successfully parsed as JSON:\", type(parsed))\n",
    "    except:\n",
    "        print(\"Not valid JSON\")\n",
    "elif isinstance(emdat[\"Admin Units\"].iloc[0], list):\n",
    "    print(\"\\nIt's already a list\")\n",
    "    print(\"List element types:\", [type(item) for item in emdat[\"Admin Units\"].iloc[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== EXTRACTION RESULTS ===\n",
      "Total rows processed: 43\n",
      "Rows with adm1 names: 27\n",
      "Rows with adm2 names: 17\n",
      "\n",
      "=== SAMPLE RESULTS ===\n",
      "Row 0:\n",
      "  adm1_names: ['Catamarca', 'Cordoba', 'Jujuy', 'La Rioja', 'Salta', 'Santiago Del Estero', 'Tucuman']\n",
      "  adm2_names: []\n",
      "\n",
      "Row 1:\n",
      "  adm1_names: ['Buenos Aires D.f.', 'Cordoba', 'La Pampa']\n",
      "  adm2_names: ['Avellaneda', 'Berisso', 'Lanus', 'Quilmes', 'San Miguel', 'San  Fernando', 'Parana', 'Rosario']\n",
      "\n",
      "Row 2:\n",
      "  adm1_names: ['Buenos Aires', 'Cordoba', 'La Pampa', 'Santa Fe']\n",
      "  adm2_names: []\n",
      "\n",
      "Row 3:\n",
      "  adm1_names: []\n",
      "  adm2_names: ['Iriondo']\n",
      "\n",
      "Row 4:\n",
      "  adm1_names: ['Buenos Aires']\n",
      "  adm2_names: []\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def extract_admin_names(admin_units_str):\n",
    "    \"\"\"\n",
    "    Extract adm1 and adm2 names from a JSON string of admin units.\n",
    "    Returns (adm1_names, adm2_names) as lists.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        parsed = json.loads(admin_units_str)\n",
    "        adm1_names = []\n",
    "        adm2_names = []\n",
    "\n",
    "        for unit in parsed:\n",
    "            if \"adm1_name\" in unit:\n",
    "                adm1_names.append(unit[\"adm1_name\"])\n",
    "            if \"adm2_name\" in unit:\n",
    "                adm2_names.append(unit[\"adm2_name\"])\n",
    "\n",
    "        return adm1_names, adm2_names\n",
    "    except:\n",
    "        # Return empty lists if parsing fails\n",
    "        return [], []\n",
    "\n",
    "\n",
    "# Apply the extraction function to create new columns\n",
    "emdat[[\"adm1_names\", \"adm2_names\"]] = emdat[\"Admin Units\"].apply(\n",
    "    lambda x: pd.Series(extract_admin_names(x))\n",
    ")\n",
    "\n",
    "# Display the results\n",
    "print(\"=== EXTRACTION RESULTS ===\")\n",
    "print(f\"Total rows processed: {len(emdat)}\")\n",
    "print(f\"Rows with adm1 names: {sum(emdat['adm1_names'].apply(len) > 0)}\")\n",
    "print(f\"Rows with adm2 names: {sum(emdat['adm2_names'].apply(len) > 0)}\")\n",
    "\n",
    "# Show a few examples\n",
    "print(\"\\n=== SAMPLE RESULTS ===\")\n",
    "for i in range(min(5, len(emdat))):\n",
    "    print(f\"Row {i}:\")\n",
    "    print(f\"  adm1_names: {emdat['adm1_names'].iloc[i]}\")\n",
    "    print(f\"  adm2_names: {emdat['adm2_names'].iloc[i]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TESTING ROW 1 ===\n",
      "adm1_names: ['Buenos Aires D.f.', 'Cordoba', 'La Pampa']\n",
      "adm2_names: ['Avellaneda', 'Berisso', 'Lanus', 'Quilmes', 'San Miguel', 'San  Fernando', 'Parana', 'Rosario']\n",
      "Filtered by 8 adm2 names: ['Avellaneda', 'Berisso', 'Lanus', 'Quilmes', 'San Miguel', 'San  Fernando', 'Parana', 'Rosario']\n",
      "\n",
      "Bounding box geometry: POLYGON ((-56.9436 -39.92531, -56.9436 -27.468575, -67.8722 -27.468575, -67.8722 -39.92531, -56.9436 -39.92531))\n",
      "Min/Max coordinates: (-67.8722, -39.92531, -56.9436, -27.468575)\n"
     ]
    }
   ],
   "source": [
    "from shapely.geometry import box\n",
    "from utils.pygeoboundaries.main import get_area_of_interest_by_names\n",
    "\n",
    "\n",
    "def get_flood_bounding_box(emdat_row, country_iso3):\n",
    "    \"\"\"\n",
    "    Get bounding box for a flood event based on admin unit names.\n",
    "\n",
    "    Args:\n",
    "        emdat_row: Single row from emdat dataframe with adm1_names and adm2_names columns\n",
    "        country_iso3: ISO3 country code for filtering\n",
    "\n",
    "    Returns:\n",
    "        bounding_box: shapely geometry representing the bounding box\n",
    "        bbox_dict: dictionary with bounding box coordinates\n",
    "    \"\"\"\n",
    "\n",
    "    # Check if we have adm2 names (prefer adm2 over adm1 for more precision)\n",
    "    if len(emdat_row[\"adm2_names\"]) > 0:\n",
    "        adm2_names = emdat_row[\"adm2_names\"]\n",
    "        print(f\"Filtered by {len(adm2_names)} adm2 names: {adm2_names}\")\n",
    "\n",
    "        # Get bounding box using geoBoundaries\n",
    "        bbox_dict = get_area_of_interest_by_names(\n",
    "            unit_names=adm2_names, adm_level=\"ADM2\", country_iso3=country_iso3\n",
    "        )\n",
    "\n",
    "    elif len(emdat_row[\"adm1_names\"]) > 0:\n",
    "        adm1_names = emdat_row[\"adm1_names\"]\n",
    "        print(f\"Filtered by {len(adm1_names)} adm1 names: {adm1_names}\")\n",
    "\n",
    "        # Get bounding box using geoBoundaries\n",
    "        bbox_dict = get_area_of_interest_by_names(\n",
    "            unit_names=adm1_names, adm_level=\"ADM1\", country_iso3=country_iso3\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        print(\"No admin names found!\")\n",
    "        return None, None\n",
    "\n",
    "    if bbox_dict is None:\n",
    "        print(\"No matching administrative units found!\")\n",
    "        return None, None\n",
    "\n",
    "    # Convert dictionary to shapely geometry\n",
    "    bbox_geometry = box(\n",
    "        bbox_dict[\"min_lon\"],\n",
    "        bbox_dict[\"min_lat\"],\n",
    "        bbox_dict[\"max_lon\"],\n",
    "        bbox_dict[\"max_lat\"],\n",
    "    )\n",
    "\n",
    "    # print(f\"Bounding box coordinates: {bbox_dict}\")\n",
    "    # print(f\"Bounding box area: {bbox_geometry.area:.6f}\")\n",
    "\n",
    "    return bbox_geometry, bbox_dict\n",
    "\n",
    "\n",
    "# Example usage for a single row\n",
    "# Assuming you have the country ISO3 code (e.g., 'ARG' for Argentina)\n",
    "country_iso3 = \"ARG\"  # You'll need to get this from your emdat data\n",
    "\n",
    "# Test with row 1 (which has both adm1 and adm2 names)\n",
    "test_row = emdat.iloc[1]\n",
    "print(\"=== TESTING ROW 1 ===\")\n",
    "print(f\"adm1_names: {test_row['adm1_names']}\")\n",
    "print(f\"adm2_names: {test_row['adm2_names']}\")\n",
    "\n",
    "bbox, bbox_dict = get_flood_bounding_box(test_row, country_iso3)\n",
    "\n",
    "if bbox is not None:\n",
    "    print(f\"\\nBounding box geometry: {bbox}\")\n",
    "\n",
    "    # You can also get the coordinates for satellite imagery search\n",
    "    bounds = bbox.bounds\n",
    "    print(f\"Min/Max coordinates: {bounds}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATE PROCESSING RESULTS ===\n",
      "Total rows: 43\n",
      "Rows with valid start_date: 40\n",
      "Rows with valid end_date: 41\n",
      "Rows with valid datetime_range: 41\n",
      "\n",
      "=== SAMPLE RESULTS ===\n",
      "   Start Year  Start Month  Start Day  End Year  End Month  End Day  \\\n",
      "0        2000            3        9.0      2000          3     17.0   \n",
      "1        2000            5       15.0      2000          5     15.0   \n",
      "2        2000           11        9.0      2000         11     16.0   \n",
      "3        2000           11       22.0      2000         11     22.0   \n",
      "4        2001            3       21.0      2001          3     21.0   \n",
      "5        2001            7        NaN      2001          7      NaN   \n",
      "6        2001           10        1.0      2001         12      1.0   \n",
      "7        2002           10       12.0      2002         11      2.0   \n",
      "8        2003            2       10.0      2003          2     10.0   \n",
      "9        2003            4       28.0      2003          5     10.0   \n",
      "\n",
      "   start_date    end_date         datetime_range  \n",
      "0  2000-03-09  2000-03-17  2000-03-17/2000-03-19  \n",
      "1  2000-05-15  2000-05-15  2000-05-15/2000-05-17  \n",
      "2  2000-11-09  2000-11-16  2000-11-16/2000-11-18  \n",
      "3  2000-11-22  2000-11-22  2000-11-22/2000-11-24  \n",
      "4  2001-03-21  2001-03-21  2001-03-21/2001-03-23  \n",
      "5        None        None                   None  \n",
      "6  2001-10-01  2001-12-01  2001-12-01/2001-12-03  \n",
      "7  2002-10-12  2002-11-02  2002-11-02/2002-11-04  \n",
      "8  2003-02-10  2003-02-10  2003-02-10/2003-02-12  \n",
      "9  2003-04-28  2003-05-10  2003-05-10/2003-05-12  \n",
      "\n",
      "Example datetime range for row 1: 2000-05-15/2000-05-17\n",
      "This represents: 2000-05-15 to 2000-05-17\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "# Create date column from Year, Month, Day\n",
    "def create_date(row, year_col, month_col, day_col):\n",
    "    \"\"\"\n",
    "    Create date in YYYY-MM-DD format from Year, Month, Day columns.\n",
    "    Handles missing values by returning None.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        year = row[year_col]\n",
    "        month = row[month_col]\n",
    "        day = row[day_col]\n",
    "\n",
    "        # Check if all values are present and valid\n",
    "        if pd.notna(year) and pd.notna(month) and pd.notna(day):\n",
    "            # Convert to integers and create date\n",
    "            year = int(year)\n",
    "            month = int(month)\n",
    "            day = int(day)\n",
    "\n",
    "            # Validate date\n",
    "            date_obj = datetime(year, month, day)\n",
    "            return date_obj.strftime(\"%Y-%m-%d\")\n",
    "        else:\n",
    "            return None\n",
    "    except (ValueError, TypeError):\n",
    "        return None\n",
    "\n",
    "\n",
    "# Apply the function to create start_date and end_date columns\n",
    "emdat[\"start_date\"] = emdat.apply(\n",
    "    lambda row: create_date(row, \"Start Year\", \"Start Month\", \"Start Day\"), axis=1\n",
    ")\n",
    "emdat[\"end_date\"] = emdat.apply(\n",
    "    lambda row: create_date(row, \"End Year\", \"End Month\", \"End Day\"), axis=1\n",
    ")\n",
    "\n",
    "\n",
    "# Create datetime range for satellite imagery search\n",
    "def create_datetime_range(end_date_str):\n",
    "    \"\"\"\n",
    "    Create datetime range starting from end_date and ending 2 days after.\n",
    "    Format: \"YYYY-MM-DD/YYYY-MM-DD\"\n",
    "    \"\"\"\n",
    "    if end_date_str is None:\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        end_date = datetime.strptime(end_date_str, \"%Y-%m-%d\")\n",
    "        start_date = end_date\n",
    "        end_date_plus_2 = end_date + timedelta(days=2)\n",
    "\n",
    "        return (\n",
    "            f\"{start_date.strftime('%Y-%m-%d')}/{end_date_plus_2.strftime('%Y-%m-%d')}\"\n",
    "        )\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "\n",
    "# Create datetime_range column\n",
    "emdat[\"datetime_range\"] = emdat[\"end_date\"].apply(create_datetime_range)\n",
    "\n",
    "# Display results\n",
    "print(\"=== DATE PROCESSING RESULTS ===\")\n",
    "print(f\"Total rows: {len(emdat)}\")\n",
    "print(f\"Rows with valid start_date: {emdat['start_date'].notna().sum()}\")\n",
    "print(f\"Rows with valid end_date: {emdat['end_date'].notna().sum()}\")\n",
    "print(f\"Rows with valid datetime_range: {emdat['datetime_range'].notna().sum()}\")\n",
    "\n",
    "# Show sample results\n",
    "print(\"\\n=== SAMPLE RESULTS ===\")\n",
    "sample_cols = [\n",
    "    \"Start Year\",\n",
    "    \"Start Month\",\n",
    "    \"Start Day\",\n",
    "    \"End Year\",\n",
    "    \"End Month\",\n",
    "    \"End Day\",\n",
    "    \"start_date\",\n",
    "    \"end_date\",\n",
    "    \"datetime_range\",\n",
    "]\n",
    "print(emdat[sample_cols].head(10))\n",
    "\n",
    "# Example usage for a specific row\n",
    "test_row = emdat.iloc[1]\n",
    "if test_row[\"datetime_range\"] is not None:\n",
    "    print(f\"\\nExample datetime range for row 1: {test_row['datetime_range']}\")\n",
    "    print(\n",
    "        f\"This represents: {test_row['datetime_range'].split('/')[0]} to {test_row['datetime_range'].split('/')[1]}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STAC query\n",
    "Filter for Sentinel 1 and/or 2 images that are 1) within two days of the flood event, 2) below 20% cloud cover, 3) within the bounding box (duh), 4) ???."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script type=\"esms-options\">{\"shimMode\": true}</script><style>*[data-root-id],\n",
       "*[data-root-id] > * {\n",
       "  box-sizing: border-box;\n",
       "  font-family: var(--jp-ui-font-family);\n",
       "  font-size: var(--jp-ui-font-size1);\n",
       "  color: var(--vscode-editor-foreground, var(--jp-ui-font-color1));\n",
       "}\n",
       "\n",
       "/* Override VSCode background color */\n",
       ".cell-output-ipywidget-background:has(\n",
       "    > .cell-output-ipywidget-background > .lm-Widget > *[data-root-id]\n",
       "  ),\n",
       ".cell-output-ipywidget-background:has(> .lm-Widget > *[data-root-id]) {\n",
       "  background-color: transparent !important;\n",
       "}\n",
       "</style>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n  const py_version = '3.7.3'.replace('rc', '-rc.').replace('.dev', '-dev.');\n  const reloading = false;\n  const Bokeh = root.Bokeh;\n\n  // Set a timeout for this load but only if we are not already initializing\n  if (typeof (root._bokeh_timeout) === \"undefined\" || (force || !root._bokeh_is_initializing)) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks;\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, js_modules, js_exports, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n    if (js_modules == null) js_modules = [];\n    if (js_exports == null) js_exports = {};\n\n    root._bokeh_onload_callbacks.push(callback);\n\n    if (root._bokeh_is_loading > 0) {\n      // Don't load bokeh if it is still initializing\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    } else if (js_urls.length === 0 && js_modules.length === 0 && Object.keys(js_exports).length === 0) {\n      // There is nothing to load\n      run_callbacks();\n      return null;\n    }\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n    window._bokeh_on_load = on_load\n\n    function on_error(e) {\n      const src_el = e.srcElement\n      console.error(\"failed to load \" + (src_el.href || src_el.src));\n    }\n\n    const skip = [];\n    if (window.requirejs) {\n      window.requirejs.config({'packages': {}, 'paths': {}, 'shim': {}});\n      root._bokeh_is_loading = css_urls.length + 0;\n    } else {\n      root._bokeh_is_loading = css_urls.length + js_urls.length + js_modules.length + Object.keys(js_exports).length;\n    }\n\n    const existing_stylesheets = []\n    const links = document.getElementsByTagName('link')\n    for (let i = 0; i < links.length; i++) {\n      const link = links[i]\n      if (link.href != null) {\n        existing_stylesheets.push(link.href)\n      }\n    }\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const escaped = encodeURI(url)\n      if (existing_stylesheets.indexOf(escaped) !== -1) {\n        on_load()\n        continue;\n      }\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }    var existing_scripts = []\n    const scripts = document.getElementsByTagName('script')\n    for (let i = 0; i < scripts.length; i++) {\n      var script = scripts[i]\n      if (script.src != null) {\n        existing_scripts.push(script.src)\n      }\n    }\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const escaped = encodeURI(url)\n      if (skip.indexOf(escaped) !== -1 || existing_scripts.indexOf(escaped) !== -1) {\n        if (!window.requirejs) {\n          on_load();\n        }\n        continue;\n      }\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (let i = 0; i < js_modules.length; i++) {\n      const url = js_modules[i];\n      const escaped = encodeURI(url)\n      if (skip.indexOf(escaped) !== -1 || existing_scripts.indexOf(escaped) !== -1) {\n        if (!window.requirejs) {\n          on_load();\n        }\n        continue;\n      }\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (const name in js_exports) {\n      const url = js_exports[name];\n      const escaped = encodeURI(url)\n      if (skip.indexOf(escaped) >= 0 || root[name] != null) {\n        if (!window.requirejs) {\n          on_load();\n        }\n        continue;\n      }\n      var element = document.createElement('script');\n      element.onerror = on_error;\n      element.async = false;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      element.textContent = `\n      import ${name} from \"${url}\"\n      window.${name} = ${name}\n      window._bokeh_on_load()\n      `\n      document.head.appendChild(element);\n    }\n    if (!js_urls.length && !js_modules.length) {\n      on_load()\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  const js_urls = [\"https://cdn.holoviz.org/panel/1.7.2/dist/bundled/reactiveesm/es-module-shims@^1.10.0/dist/es-module-shims.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-3.7.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.7.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.7.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.7.3.min.js\", \"https://cdn.holoviz.org/panel/1.7.2/dist/panel.min.js\"];\n  const js_modules = [];\n  const js_exports = {};\n  const css_urls = [];\n  const inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {} // ensure no trailing comma for IE\n  ];\n\n  function run_inline_js() {\n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (let i = 0; i < inline_js.length; i++) {\n        try {\n          inline_js[i].call(root, root.Bokeh);\n        } catch(e) {\n          if (!reloading) {\n            throw e;\n          }\n        }\n      }\n      // Cache old bokeh versions\n      if (Bokeh != undefined && !reloading) {\n        var NewBokeh = root.Bokeh;\n        if (Bokeh.versions === undefined) {\n          Bokeh.versions = new Map();\n        }\n        if (NewBokeh.version !== Bokeh.version) {\n          Bokeh.versions.set(NewBokeh.version, NewBokeh)\n        }\n        root.Bokeh = Bokeh;\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    }\n    root._bokeh_is_initializing = false\n  }\n\n  function load_or_wait() {\n    // Implement a backoff loop that tries to ensure we do not load multiple\n    // versions of Bokeh and its dependencies at the same time.\n    // In recent versions we use the root._bokeh_is_initializing flag\n    // to determine whether there is an ongoing attempt to initialize\n    // bokeh, however for backward compatibility we also try to ensure\n    // that we do not start loading a newer (Panel>=1.0 and Bokeh>3) version\n    // before older versions are fully initialized.\n    if (root._bokeh_is_initializing && Date.now() > root._bokeh_timeout) {\n      // If the timeout and bokeh was not successfully loaded we reset\n      // everything and try loading again\n      root._bokeh_timeout = Date.now() + 5000;\n      root._bokeh_is_initializing = false;\n      root._bokeh_onload_callbacks = undefined;\n      root._bokeh_is_loading = 0\n      console.log(\"Bokeh: BokehJS was loaded multiple times but one version failed to initialize.\");\n      load_or_wait();\n    } else if (root._bokeh_is_initializing || (typeof root._bokeh_is_initializing === \"undefined\" && root._bokeh_onload_callbacks !== undefined)) {\n      setTimeout(load_or_wait, 100);\n    } else {\n      root._bokeh_is_initializing = true\n      root._bokeh_onload_callbacks = []\n      const bokeh_loaded = root.Bokeh != null && (root.Bokeh.version === py_version || (root.Bokeh.versions !== undefined && root.Bokeh.versions.has(py_version)));\n      if (!reloading && !bokeh_loaded) {\n        if (root.Bokeh) {\n          root.Bokeh = undefined;\n        }\n        console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n      }\n      load_libs(css_urls, js_urls, js_modules, js_exports, function() {\n        console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n        run_inline_js();\n      });\n    }\n  }\n  // Give older versions of the autoload script a head-start to ensure\n  // they initialize before we start loading newer version.\n  setTimeout(load_or_wait, 100)\n}(window));",
      "application/vnd.holoviews_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "\nif ((window.PyViz === undefined) || (window.PyViz instanceof HTMLElement)) {\n  window.PyViz = {comms: {}, comm_status:{}, kernels:{}, receivers: {}, plot_index: []}\n}\n\n\n    function JupyterCommManager() {\n    }\n\n    JupyterCommManager.prototype.register_target = function(plot_id, comm_id, msg_handler) {\n      if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        comm_manager.register_target(comm_id, function(comm) {\n          comm.on_msg(msg_handler);\n        });\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        window.PyViz.kernels[plot_id].registerCommTarget(comm_id, function(comm) {\n          comm.onMsg = msg_handler;\n        });\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        google.colab.kernel.comms.registerTarget(comm_id, (comm) => {\n          var messages = comm.messages[Symbol.asyncIterator]();\n          function processIteratorResult(result) {\n            var message = result.value;\n            var content = {data: message.data, comm_id};\n            var buffers = []\n            for (var buffer of message.buffers || []) {\n              buffers.push(new DataView(buffer))\n            }\n            var metadata = message.metadata || {};\n            var msg = {content, buffers, metadata}\n            msg_handler(msg);\n            return messages.next().then(processIteratorResult);\n          }\n          return messages.next().then(processIteratorResult);\n        })\n      }\n    }\n\n    JupyterCommManager.prototype.get_client_comm = function(plot_id, comm_id, msg_handler) {\n      if (comm_id in window.PyViz.comms) {\n        return window.PyViz.comms[comm_id];\n      } else if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        var comm = comm_manager.new_comm(comm_id, {}, {}, {}, comm_id);\n        if (msg_handler) {\n          comm.on_msg(msg_handler);\n        }\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        var comm = window.PyViz.kernels[plot_id].connectToComm(comm_id);\n        let retries = 0;\n        const open = () => {\n          if (comm.active) {\n            comm.open();\n          } else if (retries > 3) {\n            console.warn('Comm target never activated')\n          } else {\n            retries += 1\n            setTimeout(open, 500)\n          }\n        }\n        if (comm.active) {\n          comm.open();\n        } else {\n          setTimeout(open, 500)\n        }\n        if (msg_handler) {\n          comm.onMsg = msg_handler;\n        }\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        var comm_promise = google.colab.kernel.comms.open(comm_id)\n        comm_promise.then((comm) => {\n          window.PyViz.comms[comm_id] = comm;\n          if (msg_handler) {\n            var messages = comm.messages[Symbol.asyncIterator]();\n            function processIteratorResult(result) {\n              var message = result.value;\n              var content = {data: message.data};\n              var metadata = message.metadata || {comm_id};\n              var msg = {content, metadata}\n              msg_handler(msg);\n              return messages.next().then(processIteratorResult);\n            }\n            return messages.next().then(processIteratorResult);\n          }\n        })\n        var sendClosure = (data, metadata, buffers, disposeOnDone) => {\n          return comm_promise.then((comm) => {\n            comm.send(data, metadata, buffers, disposeOnDone);\n          });\n        };\n        var comm = {\n          send: sendClosure\n        };\n      }\n      window.PyViz.comms[comm_id] = comm;\n      return comm;\n    }\n    window.PyViz.comm_manager = new JupyterCommManager();\n    \n\n\nvar JS_MIME_TYPE = 'application/javascript';\nvar HTML_MIME_TYPE = 'text/html';\nvar EXEC_MIME_TYPE = 'application/vnd.holoviews_exec.v0+json';\nvar CLASS_NAME = 'output';\n\n/**\n * Render data to the DOM node\n */\nfunction render(props, node) {\n  var div = document.createElement(\"div\");\n  var script = document.createElement(\"script\");\n  node.appendChild(div);\n  node.appendChild(script);\n}\n\n/**\n * Handle when a new output is added\n */\nfunction handle_add_output(event, handle) {\n  var output_area = handle.output_area;\n  var output = handle.output;\n  if ((output.data == undefined) || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n    return\n  }\n  var id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n  var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n  if (id !== undefined) {\n    var nchildren = toinsert.length;\n    var html_node = toinsert[nchildren-1].children[0];\n    html_node.innerHTML = output.data[HTML_MIME_TYPE];\n    var scripts = [];\n    var nodelist = html_node.querySelectorAll(\"script\");\n    for (var i in nodelist) {\n      if (nodelist.hasOwnProperty(i)) {\n        scripts.push(nodelist[i])\n      }\n    }\n\n    scripts.forEach( function (oldScript) {\n      var newScript = document.createElement(\"script\");\n      var attrs = [];\n      var nodemap = oldScript.attributes;\n      for (var j in nodemap) {\n        if (nodemap.hasOwnProperty(j)) {\n          attrs.push(nodemap[j])\n        }\n      }\n      attrs.forEach(function(attr) { newScript.setAttribute(attr.name, attr.value) });\n      newScript.appendChild(document.createTextNode(oldScript.innerHTML));\n      oldScript.parentNode.replaceChild(newScript, oldScript);\n    });\n    if (JS_MIME_TYPE in output.data) {\n      toinsert[nchildren-1].children[1].textContent = output.data[JS_MIME_TYPE];\n    }\n    output_area._hv_plot_id = id;\n    if ((window.Bokeh !== undefined) && (id in Bokeh.index)) {\n      window.PyViz.plot_index[id] = Bokeh.index[id];\n    } else {\n      window.PyViz.plot_index[id] = null;\n    }\n  } else if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n    var bk_div = document.createElement(\"div\");\n    bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n    var script_attrs = bk_div.children[0].attributes;\n    for (var i = 0; i < script_attrs.length; i++) {\n      toinsert[toinsert.length - 1].childNodes[1].setAttribute(script_attrs[i].name, script_attrs[i].value);\n    }\n    // store reference to server id on output_area\n    output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n  }\n}\n\n/**\n * Handle when an output is cleared or removed\n */\nfunction handle_clear_output(event, handle) {\n  var id = handle.cell.output_area._hv_plot_id;\n  var server_id = handle.cell.output_area._bokeh_server_id;\n  if (((id === undefined) || !(id in PyViz.plot_index)) && (server_id !== undefined)) { return; }\n  var comm = window.PyViz.comm_manager.get_client_comm(\"hv-extension-comm\", \"hv-extension-comm\", function () {});\n  if (server_id !== null) {\n    comm.send({event_type: 'server_delete', 'id': server_id});\n    return;\n  } else if (comm !== null) {\n    comm.send({event_type: 'delete', 'id': id});\n  }\n  delete PyViz.plot_index[id];\n  if ((window.Bokeh !== undefined) & (id in window.Bokeh.index)) {\n    var doc = window.Bokeh.index[id].model.document\n    doc.clear();\n    const i = window.Bokeh.documents.indexOf(doc);\n    if (i > -1) {\n      window.Bokeh.documents.splice(i, 1);\n    }\n  }\n}\n\n/**\n * Handle kernel restart event\n */\nfunction handle_kernel_cleanup(event, handle) {\n  delete PyViz.comms[\"hv-extension-comm\"];\n  window.PyViz.plot_index = {}\n}\n\n/**\n * Handle update_display_data messages\n */\nfunction handle_update_output(event, handle) {\n  handle_clear_output(event, {cell: {output_area: handle.output_area}})\n  handle_add_output(event, handle)\n}\n\nfunction register_renderer(events, OutputArea) {\n  function append_mime(data, metadata, element) {\n    // create a DOM node to render to\n    var toinsert = this.create_output_subarea(\n    metadata,\n    CLASS_NAME,\n    EXEC_MIME_TYPE\n    );\n    this.keyboard_manager.register_events(toinsert);\n    // Render to node\n    var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n    render(props, toinsert[0]);\n    element.append(toinsert);\n    return toinsert\n  }\n\n  events.on('output_added.OutputArea', handle_add_output);\n  events.on('output_updated.OutputArea', handle_update_output);\n  events.on('clear_output.CodeCell', handle_clear_output);\n  events.on('delete.Cell', handle_clear_output);\n  events.on('kernel_ready.Kernel', handle_kernel_cleanup);\n\n  OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n    safe: true,\n    index: 0\n  });\n}\n\nif (window.Jupyter !== undefined) {\n  try {\n    var events = require('base/js/events');\n    var OutputArea = require('notebook/js/outputarea').OutputArea;\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  } catch(err) {\n  }\n}\n",
      "application/vnd.holoviews_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.holoviews_exec.v0+json": "",
      "text/html": [
       "<div id='a347a974-675a-493a-8a0a-84d94914cf20'>\n",
       "  <div id=\"e0f9f1d4-06c6-4fd3-9762-0ac8bc51b23e\" data-root-id=\"a347a974-675a-493a-8a0a-84d94914cf20\" style=\"display: contents;\"></div>\n",
       "</div>\n",
       "<script type=\"application/javascript\">(function(root) {\n",
       "  var docs_json = {\"f9b2d3b3-79ad-4c97-892b-f2d8e1932b9b\":{\"version\":\"3.7.3\",\"title\":\"Bokeh Application\",\"roots\":[{\"type\":\"object\",\"name\":\"panel.models.browser.BrowserInfo\",\"id\":\"a347a974-675a-493a-8a0a-84d94914cf20\"},{\"type\":\"object\",\"name\":\"panel.models.comm_manager.CommManager\",\"id\":\"d0325161-d7a7-4ab8-b4cd-1e5d9d52a6e8\",\"attributes\":{\"plot_id\":\"a347a974-675a-493a-8a0a-84d94914cf20\",\"comm_id\":\"fd91fafcb3d24ad19d8e2816a6b21272\",\"client_comm_id\":\"8c708273b1364d09acbc7be514e0b72e\"}}],\"defs\":[{\"type\":\"model\",\"name\":\"ReactiveHTML1\"},{\"type\":\"model\",\"name\":\"FlexBox1\",\"properties\":[{\"name\":\"align_content\",\"kind\":\"Any\",\"default\":\"flex-start\"},{\"name\":\"align_items\",\"kind\":\"Any\",\"default\":\"flex-start\"},{\"name\":\"flex_direction\",\"kind\":\"Any\",\"default\":\"row\"},{\"name\":\"flex_wrap\",\"kind\":\"Any\",\"default\":\"wrap\"},{\"name\":\"gap\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"justify_content\",\"kind\":\"Any\",\"default\":\"flex-start\"}]},{\"type\":\"model\",\"name\":\"FloatPanel1\",\"properties\":[{\"name\":\"config\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"contained\",\"kind\":\"Any\",\"default\":true},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"right-top\"},{\"name\":\"offsetx\",\"kind\":\"Any\",\"default\":null},{\"name\":\"offsety\",\"kind\":\"Any\",\"default\":null},{\"name\":\"theme\",\"kind\":\"Any\",\"default\":\"primary\"},{\"name\":\"status\",\"kind\":\"Any\",\"default\":\"normalized\"}]},{\"type\":\"model\",\"name\":\"GridStack1\",\"properties\":[{\"name\":\"ncols\",\"kind\":\"Any\",\"default\":null},{\"name\":\"nrows\",\"kind\":\"Any\",\"default\":null},{\"name\":\"allow_resize\",\"kind\":\"Any\",\"default\":true},{\"name\":\"allow_drag\",\"kind\":\"Any\",\"default\":true},{\"name\":\"state\",\"kind\":\"Any\",\"default\":[]}]},{\"type\":\"model\",\"name\":\"drag1\",\"properties\":[{\"name\":\"slider_width\",\"kind\":\"Any\",\"default\":5},{\"name\":\"slider_color\",\"kind\":\"Any\",\"default\":\"black\"},{\"name\":\"value\",\"kind\":\"Any\",\"default\":50}]},{\"type\":\"model\",\"name\":\"click1\",\"properties\":[{\"name\":\"terminal_output\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"debug_name\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"clears\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"ReactiveESM1\",\"properties\":[{\"name\":\"esm_constants\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}}]},{\"type\":\"model\",\"name\":\"JSComponent1\",\"properties\":[{\"name\":\"esm_constants\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}}]},{\"type\":\"model\",\"name\":\"ReactComponent1\",\"properties\":[{\"name\":\"use_shadow_dom\",\"kind\":\"Any\",\"default\":true},{\"name\":\"esm_constants\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}}]},{\"type\":\"model\",\"name\":\"AnyWidgetComponent1\",\"properties\":[{\"name\":\"use_shadow_dom\",\"kind\":\"Any\",\"default\":true},{\"name\":\"esm_constants\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}}]},{\"type\":\"model\",\"name\":\"FastWrapper1\",\"properties\":[{\"name\":\"object\",\"kind\":\"Any\",\"default\":null},{\"name\":\"style\",\"kind\":\"Any\",\"default\":null}]},{\"type\":\"model\",\"name\":\"NotificationArea1\",\"properties\":[{\"name\":\"js_events\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"max_notifications\",\"kind\":\"Any\",\"default\":5},{\"name\":\"notifications\",\"kind\":\"Any\",\"default\":[]},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"bottom-right\"},{\"name\":\"_clear\",\"kind\":\"Any\",\"default\":0},{\"name\":\"types\",\"kind\":\"Any\",\"default\":[{\"type\":\"map\",\"entries\":[[\"type\",\"warning\"],[\"background\",\"#ffc107\"],[\"icon\",{\"type\":\"map\",\"entries\":[[\"className\",\"fas fa-exclamation-triangle\"],[\"tagName\",\"i\"],[\"color\",\"white\"]]}]]},{\"type\":\"map\",\"entries\":[[\"type\",\"info\"],[\"background\",\"#007bff\"],[\"icon\",{\"type\":\"map\",\"entries\":[[\"className\",\"fas fa-info-circle\"],[\"tagName\",\"i\"],[\"color\",\"white\"]]}]]}]}]},{\"type\":\"model\",\"name\":\"Notification\",\"properties\":[{\"name\":\"background\",\"kind\":\"Any\",\"default\":null},{\"name\":\"duration\",\"kind\":\"Any\",\"default\":3000},{\"name\":\"icon\",\"kind\":\"Any\",\"default\":null},{\"name\":\"message\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"notification_type\",\"kind\":\"Any\",\"default\":null},{\"name\":\"_rendered\",\"kind\":\"Any\",\"default\":false},{\"name\":\"_destroyed\",\"kind\":\"Any\",\"default\":false}]},{\"type\":\"model\",\"name\":\"TemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"BootstrapTemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"TemplateEditor1\",\"properties\":[{\"name\":\"layout\",\"kind\":\"Any\",\"default\":[]}]},{\"type\":\"model\",\"name\":\"MaterialTemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"request_value1\",\"properties\":[{\"name\":\"fill\",\"kind\":\"Any\",\"default\":\"none\"},{\"name\":\"_synced\",\"kind\":\"Any\",\"default\":null},{\"name\":\"_request_sync\",\"kind\":\"Any\",\"default\":0}]}]}};\n",
       "  var render_items = [{\"docid\":\"f9b2d3b3-79ad-4c97-892b-f2d8e1932b9b\",\"roots\":{\"a347a974-675a-493a-8a0a-84d94914cf20\":\"e0f9f1d4-06c6-4fd3-9762-0ac8bc51b23e\"},\"root_ids\":[\"a347a974-675a-493a-8a0a-84d94914cf20\"]}];\n",
       "  var docs = Object.values(docs_json)\n",
       "  if (!docs) {\n",
       "    return\n",
       "  }\n",
       "  const py_version = docs[0].version.replace('rc', '-rc.').replace('.dev', '-dev.')\n",
       "  async function embed_document(root) {\n",
       "    var Bokeh = get_bokeh(root)\n",
       "    await Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "    for (const render_item of render_items) {\n",
       "      for (const root_id of render_item.root_ids) {\n",
       "\tconst id_el = document.getElementById(root_id)\n",
       "\tif (id_el.children.length && id_el.children[0].hasAttribute('data-root-id')) {\n",
       "\t  const root_el = id_el.children[0]\n",
       "\t  root_el.id = root_el.id + '-rendered'\n",
       "\t  for (const child of root_el.children) {\n",
       "            // Ensure JupyterLab does not capture keyboard shortcuts\n",
       "            // see: https://jupyterlab.readthedocs.io/en/4.1.x/extension/notebook.html#keyboard-interaction-model\n",
       "\t    child.setAttribute('data-lm-suppress-shortcuts', 'true')\n",
       "\t  }\n",
       "\t}\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  function get_bokeh(root) {\n",
       "    if (root.Bokeh === undefined) {\n",
       "      return null\n",
       "    } else if (root.Bokeh.version !== py_version) {\n",
       "      if (root.Bokeh.versions === undefined || !root.Bokeh.versions.has(py_version)) {\n",
       "\treturn null\n",
       "      }\n",
       "      return root.Bokeh.versions.get(py_version);\n",
       "    } else if (root.Bokeh.version === py_version) {\n",
       "      return root.Bokeh\n",
       "    }\n",
       "    return null\n",
       "  }\n",
       "  function is_loaded(root) {\n",
       "    var Bokeh = get_bokeh(root)\n",
       "    return (Bokeh != null && Bokeh.Panel !== undefined)\n",
       "  }\n",
       "  if (is_loaded(root)) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (is_loaded(root)) {\n",
       "        clearInterval(timer);\n",
       "        embed_document(root);\n",
       "      } else if (document.readyState == \"complete\") {\n",
       "        attempts++;\n",
       "        if (attempts > 200) {\n",
       "          clearInterval(timer);\n",
       "\t  var Bokeh = get_bokeh(root)\n",
       "\t  if (Bokeh == null || Bokeh.Panel == null) {\n",
       "            console.warn(\"Panel: ERROR: Unable to run Panel code because Bokeh or Panel library is missing\");\n",
       "\t  } else {\n",
       "\t    console.warn(\"Panel: WARNING: Attempting to render but not all required libraries could be resolved.\")\n",
       "\t    embed_document(root)\n",
       "\t  }\n",
       "        }\n",
       "      }\n",
       "    }, 25, root)\n",
       "  }\n",
       "})(window);</script>"
      ]
     },
     "metadata": {
      "application/vnd.holoviews_exec.v0+json": {
       "id": "a347a974-675a-493a-8a0a-84d94914cf20"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import planetary_computer\n",
    "import pystac_client\n",
    "import panel as pn\n",
    "\n",
    "# Enable Panel for interactive visualizations\n",
    "pn.extension()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Connect to Planetary Computer Catalog\n",
    "catalog = pystac_client.Client.open(\n",
    "    \"https://planetarycomputer.microsoft.com/api/stac/v1\",\n",
    "    modifier=planetary_computer.sign_inplace,  # Automatically signs requests\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FILTERING RESULTS ===\n",
      "Original rows: 43\n",
      "Rows with valid end_date: 41\n",
      "Rows after 2014 filter: 23\n",
      "\n",
      "=== SEARCHING SENTINEL-1 RTC FOR ALL EMDAT ROWS ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows:   0%|          | 0/23 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered by 7 adm1 names: ['Catamarca', 'Cordoba', 'Entre Rios', 'Neuquen', 'Rio Negro', 'Santa Fe', 'Santiago Del Estero']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows:   4%|▍         | 1/23 [00:00<00:17,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered by 4 adm1 names: ['Chaco', 'Corrientes', 'Formosa', 'Misiones']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows:   9%|▊         | 2/23 [00:01<00:11,  1.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered by 2 adm1 names: ['Buenos Aires', 'Buenos Aires D.f.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows:  13%|█▎        | 3/23 [00:02<00:19,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered by 9 adm1 names: ['Catamarca', 'Chaco', 'Cordoba', 'Corrientes', 'Salta', 'San Luis', 'Santa Fe', 'Santiago Del Estero', 'Tucuman']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows:  17%|█▋        | 4/23 [00:03<00:14,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered by 20 adm2 names: ['Arrecifes', 'Berisso', 'Campana', 'Chacabuco', 'Chivilcoy', 'General Pueyrredon', 'Junin', 'Lujan', 'Mercedes', 'Pergamino', 'Pila', 'Quilmes', 'Salto', 'San Andres de Giles', 'San Antonio de Areco', 'La Plata', 'Tres de Febrero', 'Zarate', 'Pilar', 'General  Lopez']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows:  22%|██▏       | 5/23 [00:04<00:14,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered by 10 adm2 names: ['Curuzu Cuatia', 'Esquina', 'Goya', 'Lavalle', 'Paso de los Libres', 'San Cosme', 'Gualeguaychu', 'Parana', 'Colon', 'Concordia']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows:  26%|██▌       | 6/23 [00:04<00:14,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered by 8 adm1 names: ['Buenos Aires', 'Chaco', 'Cordoba', 'Corrientes', 'Entre Rios', 'Formosa', 'Santa Fe', 'Santiago Del Estero']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows:  30%|███       | 7/23 [00:05<00:12,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered by 19 adm2 names: ['Florentino Ameghino', 'Baradero', 'Arrecifes', 'Colon', 'General Villegas', 'Pergamino', 'Ramallo', 'Rojas', 'Salto', 'San Antonio de Areco', 'San Nicolas', 'Colon', 'General Roca', 'Punilla', 'San Javier', 'La Paz', 'Parana', 'General  Lopez', 'Rosario']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows:  35%|███▍      | 8/23 [00:06<00:12,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered by 15 adm1 names: ['Buenos Aires', 'Catamarca', 'Chubut', 'Cordoba', 'Formosa', 'Jujuy', 'La Pampa', 'Mendoza', 'Misiones', 'Salta', 'San Juan', 'Santa Cruz', 'Santa Fe', 'Santiago Del Estero', 'Tucuman']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows:  39%|███▉      | 9/23 [00:06<00:10,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered by 2 adm1 names: ['Corrientes', 'Entre Rios']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows:  43%|████▎     | 10/23 [00:07<00:07,  1.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered by 1 adm2 names: ['Escalante']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows:  48%|████▊     | 11/23 [00:08<00:07,  1.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered by 2 adm1 names: ['Chaco', 'Salta']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows:  52%|█████▏    | 12/23 [00:08<00:06,  1.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered by 1 adm2 names: ['Presidente Roque Saenz Pena']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows:  57%|█████▋    | 13/23 [00:09<00:05,  1.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered by 7 adm2 names: ['Arrecifes', 'La Matanza', 'Lanus', 'Lobos', 'Lomas de Zamora', 'Marcos Paz', 'La Plata']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows:  61%|██████    | 14/23 [00:10<00:06,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered by 6 adm1 names: ['Chaco', 'Corrientes', 'Entre Rios', 'Santa Fe', 'Santiago Del Estero', 'Tucuman']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows:  65%|██████▌   | 15/23 [00:10<00:05,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered by 3 adm1 names: ['Chaco', 'Corrientes', 'Formosa']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows:  70%|██████▉   | 16/23 [00:11<00:04,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered by 3 adm2 names: ['Belgrano', 'General Taboada', 'Juan F. Ibarra']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows:  74%|███████▍  | 17/23 [00:12<00:04,  1.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered by 4 adm1 names: ['Chaco', 'La Rioja', 'Salta', 'Tucuman']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows:  78%|███████▊  | 18/23 [00:12<00:03,  1.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered by 1 adm1 names: ['Catamarca']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows:  83%|████████▎ | 19/23 [00:13<00:02,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered by 1 adm2 names: ['Quilmes']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows: 100%|██████████| 23/23 [00:14<00:00,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No admin names found!\n",
      "No admin names found!\n",
      "No admin names found!\n",
      "\n",
      "Total rows: 23\n",
      "Rows with imagery: 16\n",
      "Total items found: 211\n",
      "\n",
      "Rows with imagery:\n",
      "  Row 22: 2 items (2014-11-04/2014-11-06)\n",
      "  Row 23: 21 items (2015-03-04/2015-03-06)\n",
      "  Row 24: 5 items (2015-08-12/2015-08-14)\n",
      "  Row 25: 6 items (2016-01-11/2016-01-13)\n",
      "  Row 26: 19 items (2016-04-15/2016-04-17)\n",
      "  Row 27: 26 items (2016-12-26/2016-12-28)\n",
      "  Row 28: 48 items (2017-04-07/2017-04-09)\n",
      "  Row 29: 5 items (2017-06-14/2017-06-16)\n",
      "  Row 31: 16 items (2018-02-21/2018-02-23)\n",
      "  Row 33: 3 items (2018-11-12/2018-11-14)\n",
      "  Row 34: 20 items (2019-01-17/2019-01-19)\n",
      "  Row 35: 12 items (2019-04-23/2019-04-25)\n",
      "  Row 36: 3 items (2019-05-20/2019-05-22)\n",
      "  Row 37: 13 items (2020-02-19/2020-02-21)\n",
      "  Row 38: 11 items (2021-03-01/2021-03-03)\n",
      "  Row 39: 1 items (2023-07-05/2023-07-07)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Filter out observations before 2014\n",
    "emdat_filtered = emdat[emdat[\"end_date\"].notna()].copy()\n",
    "emdat_filtered[\"end_date_obj\"] = pd.to_datetime(emdat_filtered[\"end_date\"])\n",
    "emdat_filtered = emdat_filtered[emdat_filtered[\"end_date_obj\"] >= \"2014-01-01\"]\n",
    "\n",
    "print(\"=== FILTERING RESULTS ===\")\n",
    "print(f\"Original rows: {len(emdat)}\")\n",
    "print(f\"Rows with valid end_date: {len(emdat[emdat['end_date'].notna()])}\")\n",
    "print(f\"Rows after 2014 filter: {len(emdat_filtered)}\")\n",
    "\n",
    "# Initialize results storage\n",
    "results = []\n",
    "\n",
    "print(\"\\n=== SEARCHING SENTINEL-1 RTC FOR ALL EMDAT ROWS ===\")\n",
    "\n",
    "# Process each row\n",
    "for idx, row in tqdm(\n",
    "    emdat_filtered.iterrows(), total=len(emdat_filtered), desc=\"Processing rows\"\n",
    "):\n",
    "    try:\n",
    "        # Get bounding box for this row\n",
    "        bbox_geometry, bbox_dict = get_flood_bounding_box(row, \"ARG\")\n",
    "\n",
    "        if bbox_geometry is None:\n",
    "            results.append(\n",
    "                {\n",
    "                    \"row_index\": idx,\n",
    "                    \"event_name\": row.get(\"Event Name\", \"Unknown\"),\n",
    "                    \"end_date\": row.get(\"end_date\", None),\n",
    "                    \"datetime_range\": row.get(\"datetime_range\", None),\n",
    "                    \"bbox_available\": False,\n",
    "                    \"items_found\": 0,\n",
    "                    \"error\": \"No bounding box available\",\n",
    "                }\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        # Convert shapely geometry bounds to bbox format\n",
    "        bounds = bbox_geometry.bounds\n",
    "        bbox = [bounds[0], bounds[1], bounds[2], bounds[3]]\n",
    "\n",
    "        # Get datetime range\n",
    "        datetime_range = row.get(\"datetime_range\")\n",
    "\n",
    "        if datetime_range is None:\n",
    "            results.append(\n",
    "                {\n",
    "                    \"row_index\": idx,\n",
    "                    \"event_name\": row.get(\"Event Name\", \"Unknown\"),\n",
    "                    \"end_date\": row.get(\"end_date\", None),\n",
    "                    \"datetime_range\": None,\n",
    "                    \"bbox_available\": True,\n",
    "                    \"items_found\": 0,\n",
    "                    \"error\": \"No valid date range\",\n",
    "                }\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        # Search for Sentinel-1 RTC data\n",
    "        search = catalog.search(\n",
    "            collections=[\"sentinel-1-rtc\"],\n",
    "            bbox=bbox,\n",
    "            datetime=datetime_range,\n",
    "            query={\n",
    "                \"sar:instrument_mode\": {\"eq\": \"IW\"},\n",
    "                \"sar:frequency_band\": {\"eq\": \"C\"},\n",
    "                \"sar:polarizations\": {\"in\": [[\"VV\"], [\"VH\"], [\"VV\", \"VH\"]]},\n",
    "            },\n",
    "        )\n",
    "\n",
    "        items = search.item_collection()\n",
    "\n",
    "        # Store results\n",
    "        results.append(\n",
    "            {\n",
    "                \"row_index\": idx,\n",
    "                \"event_name\": row.get(\"Event Name\", \"Unknown\"),\n",
    "                \"end_date\": row.get(\"end_date\", None),\n",
    "                \"datetime_range\": datetime_range,\n",
    "                \"bbox_available\": True,\n",
    "                \"items_found\": len(items),\n",
    "                \"bbox_coords\": bbox,\n",
    "                \"error\": None,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        results.append(\n",
    "            {\n",
    "                \"row_index\": idx,\n",
    "                \"event_name\": row.get(\"Event Name\", \"Unknown\"),\n",
    "                \"end_date\": row.get(\"end_date\", None),\n",
    "                \"datetime_range\": row.get(\"datetime_range\", None),\n",
    "                \"bbox_available\": False,\n",
    "                \"items_found\": 0,\n",
    "                \"error\": str(e),\n",
    "            }\n",
    "        )\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\nTotal rows: {len(results_df)}\")\n",
    "print(f\"Rows with imagery: {(results_df['items_found'] > 0).sum()}\")\n",
    "print(f\"Total items found: {results_df['items_found'].sum()}\")\n",
    "\n",
    "# Show rows with imagery\n",
    "rows_with_imagery = results_df[results_df[\"items_found\"] > 0]\n",
    "if len(rows_with_imagery) > 0:\n",
    "    print(\"\\nRows with imagery:\")\n",
    "    for _, row in rows_with_imagery.iterrows():\n",
    "        print(\n",
    "            f\"  Row {row['row_index']}: {row['items_found']} items ({row['datetime_range']})\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FILTERING RESULTS ===\n",
      "Original rows: 43\n",
      "Rows with valid end_date: 41\n",
      "Rows after 2014 filter: 23\n",
      "=== SEARCHING SENTINEL-2 L2A FOR ALL EMDAT ROWS ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows:   0%|          | 0/23 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered by 7 adm1 names: ['Catamarca', 'Cordoba', 'Entre Rios', 'Neuquen', 'Rio Negro', 'Santa Fe', 'Santiago Del Estero']\n",
      "\n",
      "[Row 20] Searching Sentinel-2 L2A for event 'Event_20'\n",
      "  BBOX: [-69.114097, -34.384612, -58.807222, -25.129197]\n",
      "  Date range: 2014-04-08/2014-04-10\n",
      "  Query: {'eo:cloud_cover': {'lt': 20}, 'platform': {'in': ['sentinel-2a', 'sentinel-2b']}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows:   4%|▍         | 1/23 [00:00<00:06,  3.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Items found: 0\n",
      "Filtered by 4 adm1 names: ['Chaco', 'Corrientes', 'Formosa', 'Misiones']\n",
      "\n",
      "[Row 21] Searching Sentinel-2 L2A for event 'Event_21'\n",
      "  BBOX: [-63.271647, -30.591262, -53.601347, -22.512505]\n",
      "  Date range: 2014-06-30/2014-07-02\n",
      "  Query: {'eo:cloud_cover': {'lt': 20}, 'platform': {'in': ['sentinel-2a', 'sentinel-2b']}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows:   9%|▊         | 2/23 [00:00<00:06,  3.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Items found: 0\n",
      "Filtered by 2 adm1 names: ['Buenos Aires', 'Buenos Aires D.f.']\n",
      "\n",
      "[Row 22] Searching Sentinel-2 L2A for event 'Event_22'\n",
      "  BBOX: [-63.418455, -41.033855, -56.641502, -30.151774]\n",
      "  Date range: 2014-11-04/2014-11-06\n",
      "  Query: {'eo:cloud_cover': {'lt': 20}, 'platform': {'in': ['sentinel-2a', 'sentinel-2b']}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows:  13%|█▎        | 3/23 [00:00<00:06,  3.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Items found: 0\n",
      "Filtered by 9 adm1 names: ['Catamarca', 'Chaco', 'Cordoba', 'Corrientes', 'Salta', 'San Luis', 'Santa Fe', 'Santiago Del Estero', 'Tucuman']\n",
      "\n",
      "[Row 23] Searching Sentinel-2 L2A for event 'Event_23'\n",
      "  BBOX: [-69.114097, -36.000164, -55.609897, -22.000757]\n",
      "  Date range: 2015-03-04/2015-03-06\n",
      "  Query: {'eo:cloud_cover': {'lt': 20}, 'platform': {'in': ['sentinel-2a', 'sentinel-2b']}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows:  17%|█▋        | 4/23 [00:01<00:06,  3.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Items found: 0\n",
      "Filtered by 20 adm2 names: ['Arrecifes', 'Berisso', 'Campana', 'Chacabuco', 'Chivilcoy', 'General Pueyrredon', 'Junin', 'Lujan', 'Mercedes', 'Pergamino', 'Pila', 'Quilmes', 'Salto', 'San Andres de Giles', 'San Antonio de Areco', 'La Plata', 'Tres de Febrero', 'Zarate', 'Pilar', 'General  Lopez']\n",
      "\n",
      "[Row 24] Searching Sentinel-2 L2A for event 'Event_24'\n",
      "  BBOX: [-65.578013, -36.615345, -57.164433, -26.758911]\n",
      "  Date range: 2015-08-12/2015-08-14\n",
      "  Query: {'eo:cloud_cover': {'lt': 20}, 'platform': {'in': ['sentinel-2a', 'sentinel-2b']}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows:  22%|██▏       | 5/23 [00:02<00:09,  1.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Items found: 0\n",
      "Filtered by 10 adm2 names: ['Curuzu Cuatia', 'Esquina', 'Goya', 'Lavalle', 'Paso de los Libres', 'San Cosme', 'Gualeguaychu', 'Parana', 'Colon', 'Concordia']\n",
      "\n",
      "[Row 25] Searching Sentinel-2 L2A for event 'Event_25'\n",
      "  BBOX: [-68.695168, -33.119871, -56.838651, -27.265963]\n",
      "  Date range: 2016-01-11/2016-01-13\n",
      "  Query: {'eo:cloud_cover': {'lt': 20}, 'platform': {'in': ['sentinel-2a', 'sentinel-2b']}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows:  26%|██▌       | 6/23 [00:04<00:18,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Items found: 1\n",
      "Filtered by 8 adm1 names: ['Buenos Aires', 'Chaco', 'Cordoba', 'Corrientes', 'Entre Rios', 'Formosa', 'Santa Fe', 'Santiago Del Estero']\n",
      "\n",
      "[Row 26] Searching Sentinel-2 L2A for event 'Event_26'\n",
      "  BBOX: [-65.057829, -41.033855, -55.609897, -22.512505]\n",
      "  Date range: 2016-04-15/2016-04-17\n",
      "  Query: {'eo:cloud_cover': {'lt': 20}, 'platform': {'in': ['sentinel-2a', 'sentinel-2b']}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows:  30%|███       | 7/23 [00:05<00:19,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Items found: 40\n",
      "Filtered by 19 adm2 names: ['Florentino Ameghino', 'Baradero', 'Arrecifes', 'Colon', 'General Villegas', 'Pergamino', 'Ramallo', 'Rojas', 'Salto', 'San Antonio de Areco', 'San Nicolas', 'Colon', 'General Roca', 'Punilla', 'San Javier', 'La Paz', 'Parana', 'General  Lopez', 'Rosario']\n",
      "\n",
      "[Row 27] Searching Sentinel-2 L2A for event 'Event_27'\n",
      "  BBOX: [-68.251317, -45.131851, -54.976067, -27.615255]\n",
      "  Date range: 2016-12-26/2016-12-28\n",
      "  Query: {'eo:cloud_cover': {'lt': 20}, 'platform': {'in': ['sentinel-2a', 'sentinel-2b']}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows:  35%|███▍      | 8/23 [00:07<00:22,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Items found: 75\n",
      "Filtered by 15 adm1 names: ['Buenos Aires', 'Catamarca', 'Chubut', 'Cordoba', 'Formosa', 'Jujuy', 'La Pampa', 'Mendoza', 'Misiones', 'Salta', 'San Juan', 'Santa Cruz', 'Santa Fe', 'Santiago Del Estero', 'Tucuman']\n",
      "\n",
      "[Row 28] Searching Sentinel-2 L2A for event 'Event_28'\n",
      "  BBOX: [-73.531182, -52.366255, -53.601347, -21.805624]\n",
      "  Date range: 2017-04-07/2017-04-09\n",
      "  Query: {'eo:cloud_cover': {'lt': 20}, 'platform': {'in': ['sentinel-2a', 'sentinel-2b']}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows:  39%|███▉      | 9/23 [00:09<00:19,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Items found: 71\n",
      "Filtered by 2 adm1 names: ['Corrientes', 'Entre Rios']\n",
      "\n",
      "[Row 29] Searching Sentinel-2 L2A for event 'Event_29'\n",
      "  BBOX: [-59.620913, -30.591262, -55.609897, -27.301516]\n",
      "  Date range: 2017-06-14/2017-06-16\n",
      "  Query: {'eo:cloud_cover': {'lt': 20}, 'platform': {'in': ['sentinel-2a', 'sentinel-2b']}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows:  43%|████▎     | 10/23 [00:09<00:13,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Items found: 0\n",
      "Filtered by 1 adm2 names: ['Escalante']\n",
      "\n",
      "[Row 30] Searching Sentinel-2 L2A for event 'Event_30'\n",
      "  BBOX: [-68.371193, -45.999795, -66.360497, -44.678928]\n",
      "  Date range: 2017-04-21/2017-04-23\n",
      "  Query: {'eo:cloud_cover': {'lt': 20}, 'platform': {'in': ['sentinel-2a', 'sentinel-2b']}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows:  48%|████▊     | 11/23 [00:10<00:11,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Items found: 0\n",
      "Filtered by 2 adm1 names: ['Chaco', 'Salta']\n",
      "\n",
      "[Row 31] Searching Sentinel-2 L2A for event 'Event_31'\n",
      "  BBOX: [-68.575573, -28.020576, -58.316378, -22.000757]\n",
      "  Date range: 2018-02-21/2018-02-23\n",
      "  Query: {'eo:cloud_cover': {'lt': 20}, 'platform': {'in': ['sentinel-2a', 'sentinel-2b']}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows:  52%|█████▏    | 12/23 [00:11<00:11,  1.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Items found: 45\n",
      "Filtered by 1 adm2 names: ['Presidente Roque Saenz Pena']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows:  57%|█████▋    | 13/23 [00:11<00:08,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row 32] ERROR: No matching administrative units found for ARG at level ADM2\n",
      "Filtered by 7 adm2 names: ['Arrecifes', 'La Matanza', 'Lanus', 'Lobos', 'Lomas de Zamora', 'Marcos Paz', 'La Plata']\n",
      "\n",
      "[Row 33] Searching Sentinel-2 L2A for event 'Event_33'\n",
      "  BBOX: [-60.236801, -35.435944, -57.752731, -33.745392]\n",
      "  Date range: 2018-11-12/2018-11-14\n",
      "  Query: {'eo:cloud_cover': {'lt': 20}, 'platform': {'in': ['sentinel-2a', 'sentinel-2b']}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows:  61%|██████    | 14/23 [00:12<00:07,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Items found: 0\n",
      "Filtered by 6 adm1 names: ['Chaco', 'Corrientes', 'Entre Rios', 'Santa Fe', 'Santiago Del Estero', 'Tucuman']\n",
      "\n",
      "[Row 34] Searching Sentinel-2 L2A for event 'Event_34'\n",
      "  BBOX: [-65.057829, -34.384612, -55.609897, -24.164428]\n",
      "  Date range: 2019-01-17/2019-01-19\n",
      "  Query: {'eo:cloud_cover': {'lt': 20}, 'platform': {'in': ['sentinel-2a', 'sentinel-2b']}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows:  65%|██████▌   | 15/23 [00:13<00:06,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Items found: 43\n",
      "Filtered by 3 adm1 names: ['Chaco', 'Corrientes', 'Formosa']\n",
      "\n",
      "[Row 35] Searching Sentinel-2 L2A for event 'Event_35'\n",
      "  BBOX: [-63.271647, -30.591262, -55.609897, -22.512505]\n",
      "  Date range: 2019-04-23/2019-04-25\n",
      "  Query: {'eo:cloud_cover': {'lt': 20}, 'platform': {'in': ['sentinel-2a', 'sentinel-2b']}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows:  70%|██████▉   | 16/23 [00:13<00:05,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Items found: 14\n",
      "Filtered by 3 adm2 names: ['Belgrano', 'General Taboada', 'Juan F. Ibarra']\n",
      "\n",
      "[Row 36] Searching Sentinel-2 L2A for event 'Event_36'\n",
      "  BBOX: [-67.320481, -33.206578, -61.437693, -27.721172]\n",
      "  Date range: 2019-05-20/2019-05-22\n",
      "  Query: {'eo:cloud_cover': {'lt': 20}, 'platform': {'in': ['sentinel-2a', 'sentinel-2b']}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows:  74%|███████▍  | 17/23 [00:14<00:04,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Items found: 0\n",
      "Filtered by 4 adm1 names: ['Chaco', 'La Rioja', 'Salta', 'Tucuman']\n",
      "\n",
      "[Row 37] Searching Sentinel-2 L2A for event 'Event_37'\n",
      "  BBOX: [-68.575573, -28.020576, -58.316378, -22.000757]\n",
      "  Date range: 2020-02-19/2020-02-21\n",
      "  Query: {'eo:cloud_cover': {'lt': 20}, 'platform': {'in': ['sentinel-2a', 'sentinel-2b']}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows:  78%|███████▊  | 18/23 [00:15<00:03,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Items found: 10\n",
      "Filtered by 1 adm1 names: ['Catamarca']\n",
      "\n",
      "[Row 38] Searching Sentinel-2 L2A for event 'Event_38'\n",
      "  BBOX: [-69.114097, -30.116125, -64.839508, -25.129197]\n",
      "  Date range: 2021-03-01/2021-03-03\n",
      "  Query: {'eo:cloud_cover': {'lt': 20}, 'platform': {'in': ['sentinel-2a', 'sentinel-2b']}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows:  83%|████████▎ | 19/23 [00:15<00:02,  1.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Items found: 8\n",
      "Filtered by 1 adm2 names: ['Quilmes']\n",
      "\n",
      "[Row 39] Searching Sentinel-2 L2A for event 'Event_39'\n",
      "  BBOX: [-58.346916, -34.801014, -58.194752, -34.678736]\n",
      "  Date range: 2023-07-05/2023-07-07\n",
      "  Query: {'eo:cloud_cover': {'lt': 20}, 'platform': {'in': ['sentinel-2a', 'sentinel-2b']}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows: 100%|██████████| 23/23 [00:16<00:00,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Items found: 0\n",
      "No admin names found!\n",
      "[Row 40] No bounding box available for event 'Event_40'\n",
      "No admin names found!\n",
      "[Row 41] No bounding box available for event 'Event_41'\n",
      "No admin names found!\n",
      "[Row 42] No bounding box available for event 'Event_42'\n",
      "\n",
      "Total rows: 23\n",
      "Rows with imagery: 9\n",
      "Total items found: 307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Filter out observations before 2014\n",
    "emdat_filtered = emdat[emdat[\"end_date\"].notna()].copy()\n",
    "emdat_filtered[\"end_date_obj\"] = pd.to_datetime(emdat_filtered[\"end_date\"])\n",
    "emdat_filtered = emdat_filtered[emdat_filtered[\"end_date_obj\"] >= \"2014-01-01\"]\n",
    "\n",
    "print(\"=== FILTERING RESULTS ===\")\n",
    "print(f\"Original rows: {len(emdat)}\")\n",
    "print(f\"Rows with valid end_date: {len(emdat[emdat['end_date'].notna()])}\")\n",
    "print(f\"Rows after 2014 filter: {len(emdat_filtered)}\")\n",
    "\n",
    "# Initialize results storage\n",
    "results = []\n",
    "\n",
    "print(\"=== SEARCHING SENTINEL-2 L2A FOR ALL EMDAT ROWS ===\")\n",
    "\n",
    "# Process each row\n",
    "for idx, row in tqdm(\n",
    "    emdat_filtered.iterrows(), total=len(emdat_filtered), desc=\"Processing rows\"\n",
    "):\n",
    "    try:\n",
    "        # Get event name, fallback to row index if missing or nan\n",
    "        event_name = row.get(\"Event Name\", None)\n",
    "        if pd.isna(event_name) or not event_name:\n",
    "            event_name = f\"Event_{idx}\"\n",
    "\n",
    "        # Get bounding box for this row\n",
    "        bbox_geometry, bbox_dict = get_flood_bounding_box(row, \"ARG\")\n",
    "\n",
    "        if bbox_geometry is None:\n",
    "            print(f\"[Row {idx}] No bounding box available for event '{event_name}'\")\n",
    "            results.append(\n",
    "                {\n",
    "                    \"row_index\": idx,\n",
    "                    \"event_name\": event_name,\n",
    "                    \"end_date\": row.get(\"end_date\", None),\n",
    "                    \"datetime_range\": row.get(\"datetime_range\", None),\n",
    "                    \"bbox_available\": False,\n",
    "                    \"items_found\": 0,\n",
    "                    \"error\": \"No bounding box available\",\n",
    "                }\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        # Convert shapely geometry bounds to bbox format\n",
    "        bounds = bbox_geometry.bounds\n",
    "        bbox = [bounds[0], bounds[1], bounds[2], bounds[3]]\n",
    "\n",
    "        # Get datetime range\n",
    "        datetime_range = row.get(\"datetime_range\")\n",
    "\n",
    "        if datetime_range is None:\n",
    "            print(f\"[Row {idx}] No valid date range for event '{event_name}'\")\n",
    "            results.append(\n",
    "                {\n",
    "                    \"row_index\": idx,\n",
    "                    \"event_name\": event_name,\n",
    "                    \"end_date\": row.get(\"end_date\", None),\n",
    "                    \"datetime_range\": None,\n",
    "                    \"bbox_available\": True,\n",
    "                    \"items_found\": 0,\n",
    "                    \"error\": \"No valid date range\",\n",
    "                }\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        # Print search parameters for debugging\n",
    "        print(f\"\\n[Row {idx}] Searching Sentinel-2 L2A for event '{event_name}'\")\n",
    "        print(f\"  BBOX: {bbox}\")\n",
    "        print(f\"  Date range: {datetime_range}\")\n",
    "        print(\n",
    "            \"  Query: {'eo:cloud_cover': {'lt': 20}, 'platform': {'in': ['sentinel-2a', 'sentinel-2b']}}\"\n",
    "        )\n",
    "\n",
    "        # Search for Sentinel-2 L2A data with cloud cover filtering\n",
    "        search = catalog.search(\n",
    "            collections=[\"sentinel-2-l2a\"],\n",
    "            bbox=bbox,\n",
    "            datetime=datetime_range,\n",
    "            query={\n",
    "                \"eo:cloud_cover\": {\"lt\": 20},  # Less than 50% cloud cover\n",
    "            },\n",
    "        )\n",
    "\n",
    "        items = search.item_collection()\n",
    "        print(f\"  Items found: {len(items)}\")\n",
    "\n",
    "        # Store results\n",
    "        results.append(\n",
    "            {\n",
    "                \"row_index\": idx,\n",
    "                \"event_name\": event_name,\n",
    "                \"end_date\": row.get(\"end_date\", None),\n",
    "                \"datetime_range\": datetime_range,\n",
    "                \"bbox_available\": True,\n",
    "                \"items_found\": len(items),\n",
    "                \"bbox_coords\": bbox,\n",
    "                \"error\": None,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[Row {idx}] ERROR: {e}\")\n",
    "        results.append(\n",
    "            {\n",
    "                \"row_index\": idx,\n",
    "                \"event_name\": event_name,\n",
    "                \"end_date\": row.get(\"end_date\", None),\n",
    "                \"datetime_range\": row.get(\"datetime_range\", None),\n",
    "                \"bbox_available\": False,\n",
    "                \"items_found\": 0,\n",
    "                \"error\": str(e),\n",
    "            }\n",
    "        )\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\nTotal rows: {len(results_df)}\")\n",
    "print(f\"Rows with imagery: {(results_df['items_found'] > 0).sum()}\")\n",
    "print(f\"Total items found: {results_df['items_found'].sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_index</th>\n",
       "      <th>event_name</th>\n",
       "      <th>end_date</th>\n",
       "      <th>datetime_range</th>\n",
       "      <th>bbox_available</th>\n",
       "      <th>items_found</th>\n",
       "      <th>bbox_coords</th>\n",
       "      <th>error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>Event_20</td>\n",
       "      <td>2014-04-08</td>\n",
       "      <td>2014-04-08/2014-04-10</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>[-69.114097, -34.384612, -58.807222, -25.129197]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21</td>\n",
       "      <td>Event_21</td>\n",
       "      <td>2014-06-30</td>\n",
       "      <td>2014-06-30/2014-07-02</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>[-63.271647, -30.591262, -53.601347, -22.512505]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>Event_22</td>\n",
       "      <td>2014-11-04</td>\n",
       "      <td>2014-11-04/2014-11-06</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>[-63.418455, -41.033855, -56.641502, -30.151774]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23</td>\n",
       "      <td>Event_23</td>\n",
       "      <td>2015-03-04</td>\n",
       "      <td>2015-03-04/2015-03-06</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>[-69.114097, -36.000164, -55.609897, -22.000757]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24</td>\n",
       "      <td>Event_24</td>\n",
       "      <td>2015-08-12</td>\n",
       "      <td>2015-08-12/2015-08-14</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>[-65.578013, -36.615345, -57.164433, -26.758911]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>25</td>\n",
       "      <td>Event_25</td>\n",
       "      <td>2016-01-11</td>\n",
       "      <td>2016-01-11/2016-01-13</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>[-68.695168, -33.119871, -56.838651, -27.265963]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>26</td>\n",
       "      <td>Event_26</td>\n",
       "      <td>2016-04-15</td>\n",
       "      <td>2016-04-15/2016-04-17</td>\n",
       "      <td>True</td>\n",
       "      <td>40</td>\n",
       "      <td>[-65.057829, -41.033855, -55.609897, -22.512505]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>27</td>\n",
       "      <td>Event_27</td>\n",
       "      <td>2016-12-26</td>\n",
       "      <td>2016-12-26/2016-12-28</td>\n",
       "      <td>True</td>\n",
       "      <td>75</td>\n",
       "      <td>[-68.251317, -45.131851, -54.976067, -27.615255]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>28</td>\n",
       "      <td>Event_28</td>\n",
       "      <td>2017-04-07</td>\n",
       "      <td>2017-04-07/2017-04-09</td>\n",
       "      <td>True</td>\n",
       "      <td>71</td>\n",
       "      <td>[-73.531182, -52.366255, -53.601347, -21.805624]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>29</td>\n",
       "      <td>Event_29</td>\n",
       "      <td>2017-06-14</td>\n",
       "      <td>2017-06-14/2017-06-16</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>[-59.620913, -30.591262, -55.609897, -27.301516]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>30</td>\n",
       "      <td>Event_30</td>\n",
       "      <td>2017-04-21</td>\n",
       "      <td>2017-04-21/2017-04-23</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>[-68.371193, -45.999795, -66.360497, -44.678928]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>31</td>\n",
       "      <td>Event_31</td>\n",
       "      <td>2018-02-21</td>\n",
       "      <td>2018-02-21/2018-02-23</td>\n",
       "      <td>True</td>\n",
       "      <td>45</td>\n",
       "      <td>[-68.575573, -28.020576, -58.316378, -22.000757]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>32</td>\n",
       "      <td>Event_32</td>\n",
       "      <td>2018-01-20</td>\n",
       "      <td>2018-01-20/2018-01-22</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No matching administrative units found for ARG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>33</td>\n",
       "      <td>Event_33</td>\n",
       "      <td>2018-11-12</td>\n",
       "      <td>2018-11-12/2018-11-14</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>[-60.236801, -35.435944, -57.752731, -33.745392]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>34</td>\n",
       "      <td>Event_34</td>\n",
       "      <td>2019-01-17</td>\n",
       "      <td>2019-01-17/2019-01-19</td>\n",
       "      <td>True</td>\n",
       "      <td>43</td>\n",
       "      <td>[-65.057829, -34.384612, -55.609897, -24.164428]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>35</td>\n",
       "      <td>Event_35</td>\n",
       "      <td>2019-04-23</td>\n",
       "      <td>2019-04-23/2019-04-25</td>\n",
       "      <td>True</td>\n",
       "      <td>14</td>\n",
       "      <td>[-63.271647, -30.591262, -55.609897, -22.512505]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>36</td>\n",
       "      <td>Event_36</td>\n",
       "      <td>2019-05-20</td>\n",
       "      <td>2019-05-20/2019-05-22</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>[-67.320481, -33.206578, -61.437693, -27.721172]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>37</td>\n",
       "      <td>Event_37</td>\n",
       "      <td>2020-02-19</td>\n",
       "      <td>2020-02-19/2020-02-21</td>\n",
       "      <td>True</td>\n",
       "      <td>10</td>\n",
       "      <td>[-68.575573, -28.020576, -58.316378, -22.000757]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>38</td>\n",
       "      <td>Event_38</td>\n",
       "      <td>2021-03-01</td>\n",
       "      <td>2021-03-01/2021-03-03</td>\n",
       "      <td>True</td>\n",
       "      <td>8</td>\n",
       "      <td>[-69.114097, -30.116125, -64.839508, -25.129197]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>39</td>\n",
       "      <td>Event_39</td>\n",
       "      <td>2023-07-05</td>\n",
       "      <td>2023-07-05/2023-07-07</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>[-58.346916, -34.801014, -58.194752, -34.678736]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>40</td>\n",
       "      <td>Event_40</td>\n",
       "      <td>2024-03-03</td>\n",
       "      <td>2024-03-03/2024-03-05</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No bounding box available</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>41</td>\n",
       "      <td>Event_41</td>\n",
       "      <td>2024-05-13</td>\n",
       "      <td>2024-05-13/2024-05-15</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No bounding box available</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>42</td>\n",
       "      <td>Event_42</td>\n",
       "      <td>2025-03-10</td>\n",
       "      <td>2025-03-10/2025-03-12</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No bounding box available</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    row_index event_name    end_date         datetime_range  bbox_available  \\\n",
       "0          20   Event_20  2014-04-08  2014-04-08/2014-04-10            True   \n",
       "1          21   Event_21  2014-06-30  2014-06-30/2014-07-02            True   \n",
       "2          22   Event_22  2014-11-04  2014-11-04/2014-11-06            True   \n",
       "3          23   Event_23  2015-03-04  2015-03-04/2015-03-06            True   \n",
       "4          24   Event_24  2015-08-12  2015-08-12/2015-08-14            True   \n",
       "5          25   Event_25  2016-01-11  2016-01-11/2016-01-13            True   \n",
       "6          26   Event_26  2016-04-15  2016-04-15/2016-04-17            True   \n",
       "7          27   Event_27  2016-12-26  2016-12-26/2016-12-28            True   \n",
       "8          28   Event_28  2017-04-07  2017-04-07/2017-04-09            True   \n",
       "9          29   Event_29  2017-06-14  2017-06-14/2017-06-16            True   \n",
       "10         30   Event_30  2017-04-21  2017-04-21/2017-04-23            True   \n",
       "11         31   Event_31  2018-02-21  2018-02-21/2018-02-23            True   \n",
       "12         32   Event_32  2018-01-20  2018-01-20/2018-01-22           False   \n",
       "13         33   Event_33  2018-11-12  2018-11-12/2018-11-14            True   \n",
       "14         34   Event_34  2019-01-17  2019-01-17/2019-01-19            True   \n",
       "15         35   Event_35  2019-04-23  2019-04-23/2019-04-25            True   \n",
       "16         36   Event_36  2019-05-20  2019-05-20/2019-05-22            True   \n",
       "17         37   Event_37  2020-02-19  2020-02-19/2020-02-21            True   \n",
       "18         38   Event_38  2021-03-01  2021-03-01/2021-03-03            True   \n",
       "19         39   Event_39  2023-07-05  2023-07-05/2023-07-07            True   \n",
       "20         40   Event_40  2024-03-03  2024-03-03/2024-03-05           False   \n",
       "21         41   Event_41  2024-05-13  2024-05-13/2024-05-15           False   \n",
       "22         42   Event_42  2025-03-10  2025-03-10/2025-03-12           False   \n",
       "\n",
       "    items_found                                       bbox_coords  \\\n",
       "0             0  [-69.114097, -34.384612, -58.807222, -25.129197]   \n",
       "1             0  [-63.271647, -30.591262, -53.601347, -22.512505]   \n",
       "2             0  [-63.418455, -41.033855, -56.641502, -30.151774]   \n",
       "3             0  [-69.114097, -36.000164, -55.609897, -22.000757]   \n",
       "4             0  [-65.578013, -36.615345, -57.164433, -26.758911]   \n",
       "5             1  [-68.695168, -33.119871, -56.838651, -27.265963]   \n",
       "6            40  [-65.057829, -41.033855, -55.609897, -22.512505]   \n",
       "7            75  [-68.251317, -45.131851, -54.976067, -27.615255]   \n",
       "8            71  [-73.531182, -52.366255, -53.601347, -21.805624]   \n",
       "9             0  [-59.620913, -30.591262, -55.609897, -27.301516]   \n",
       "10            0  [-68.371193, -45.999795, -66.360497, -44.678928]   \n",
       "11           45  [-68.575573, -28.020576, -58.316378, -22.000757]   \n",
       "12            0                                               NaN   \n",
       "13            0  [-60.236801, -35.435944, -57.752731, -33.745392]   \n",
       "14           43  [-65.057829, -34.384612, -55.609897, -24.164428]   \n",
       "15           14  [-63.271647, -30.591262, -55.609897, -22.512505]   \n",
       "16            0  [-67.320481, -33.206578, -61.437693, -27.721172]   \n",
       "17           10  [-68.575573, -28.020576, -58.316378, -22.000757]   \n",
       "18            8  [-69.114097, -30.116125, -64.839508, -25.129197]   \n",
       "19            0  [-58.346916, -34.801014, -58.194752, -34.678736]   \n",
       "20            0                                               NaN   \n",
       "21            0                                               NaN   \n",
       "22            0                                               NaN   \n",
       "\n",
       "                                                error  \n",
       "0                                                None  \n",
       "1                                                None  \n",
       "2                                                None  \n",
       "3                                                None  \n",
       "4                                                None  \n",
       "5                                                None  \n",
       "6                                                None  \n",
       "7                                                None  \n",
       "8                                                None  \n",
       "9                                                None  \n",
       "10                                               None  \n",
       "11                                               None  \n",
       "12  No matching administrative units found for ARG...  \n",
       "13                                               None  \n",
       "14                                               None  \n",
       "15                                               None  \n",
       "16                                               None  \n",
       "17                                               None  \n",
       "18                                               None  \n",
       "19                                               None  \n",
       "20                          No bounding box available  \n",
       "21                          No bounding box available  \n",
       "22                          No bounding box available  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading imagery for Row 22: 2 items\n",
      "Filtered by 2 adm1 names: ['Buenos Aires', 'Buenos Aires D.f.']\n",
      "Loading item: S1A_IW_GRDH_1SSV_20141106T093826_20141106T093851_003159_003A22_rtc\n",
      "Date: 2014-11-06 09:38:38.636304+00:00\n",
      "Loading vv band...\n",
      "Saving to: /home/nissim/Documents/dev/arg-inundaciones/data/row_22_vv_20141106.tif\n",
      "Saved COG successfully!\n",
      "File size: 2595.91 MB\n",
      "Data shape: (23544, 30924)\n",
      "Data range: -32768.000000 to 1203.246826\n"
     ]
    }
   ],
   "source": [
    "import rioxarray as rio\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Get the first row with imagery from your existing results\n",
    "rows_with_imagery = results_df[results_df[\"items_found\"] > 0]\n",
    "if len(rows_with_imagery) > 0:\n",
    "    # Get the first row with imagery\n",
    "    first_row = rows_with_imagery.iloc[0]\n",
    "    row_index = first_row[\"row_index\"]\n",
    "\n",
    "    print(f\"Loading imagery for Row {row_index}: {first_row['items_found']} items\")\n",
    "\n",
    "    # Re-run just the search part to get the items\n",
    "    original_row = emdat.iloc[row_index]\n",
    "    bbox_geometry, bbox_dict = get_flood_bounding_box(original_row, \"ARG\")\n",
    "    bounds = bbox_geometry.bounds\n",
    "    bbox = [bounds[0], bounds[1], bounds[2], bounds[3]]\n",
    "    datetime_range = original_row.get(\"datetime_range\")\n",
    "\n",
    "    # Search for Sentinel-1 RTC data\n",
    "    search = catalog.search(\n",
    "        collections=[\"sentinel-1-rtc\"],\n",
    "        bbox=bbox,\n",
    "        datetime=datetime_range,\n",
    "        query={\n",
    "            \"sar:instrument_mode\": {\"eq\": \"IW\"},\n",
    "            \"sar:frequency_band\": {\"eq\": \"C\"},\n",
    "            \"sar:polarizations\": {\"in\": [[\"VV\"], [\"VH\"], [\"VV\", \"VH\"]]},\n",
    "        },\n",
    "    )\n",
    "\n",
    "    items = search.item_collection()\n",
    "\n",
    "    # Load and save the first item\n",
    "    item = items[0]\n",
    "    print(f\"Loading item: {item.id}\")\n",
    "    print(f\"Date: {item.datetime}\")\n",
    "\n",
    "    # Load the VV band (or VH if VV not available)\n",
    "    if \"vv\" in item.assets:\n",
    "        asset_key = \"vv\"\n",
    "    else:\n",
    "        asset_key = \"vh\"\n",
    "\n",
    "    print(f\"Loading {asset_key} band...\")\n",
    "    ds = rio.open_rasterio(item.assets[asset_key].href)\n",
    "\n",
    "    # Create output directory if it doesn't exist\n",
    "    output_dir = \"/home/nissim/Documents/dev/arg-inundaciones/data/\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Create filename\n",
    "    filename = f\"row_{row_index}_{asset_key}_{item.datetime.strftime('%Y%m%d')}.tif\"\n",
    "    output_path = os.path.join(output_dir, filename)\n",
    "\n",
    "    print(f\"Saving to: {output_path}\")\n",
    "\n",
    "    # Save as COG\n",
    "    ds.rio.to_raster(\n",
    "        output_path,\n",
    "        driver=\"COG\",\n",
    "        compress=\"LZW\",\n",
    "        tiled=True,\n",
    "        blockxsize=512,\n",
    "        blockysize=512,\n",
    "        overview_levels=[2, 4, 8, 16],\n",
    "        overview_resampling=\"nearest\",\n",
    "    )\n",
    "\n",
    "    print(\"Saved COG successfully!\")\n",
    "    print(f\"File size: {os.path.getsize(output_path) / (1024 * 1024):.2f} MB\")\n",
    "\n",
    "else:\n",
    "    print(\"No imagery found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nissim/Documents/dev/arg-inundaciones/.venv/lib/python3.12/site-packages/distributed/node.py:187: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 42117 instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dask dashboard: http://127.0.0.1:42117/status\n",
      "Loading Sentinel-2 imagery for Row 25: 1 items\n",
      "Filtered by 10 adm2 names: ['Curuzu Cuatia', 'Esquina', 'Goya', 'Lavalle', 'Paso de los Libres', 'San Cosme', 'Gualeguaychu', 'Parana', 'Colon', 'Concordia']\n",
      "Loading item: S2A_MSIL2A_20160113T135952_R067_T20HPJ_20210527T163414\n",
      "Date: 2016-01-13 13:59:52.029000+00:00\n",
      "Bands to load: ['B01', 'B02', 'B03', 'B04', 'B05', 'B06', 'B07', 'B08', 'B09', 'B11', 'B12', 'B8A']\n",
      "Loading bands with Dask chunking...\n",
      "  Loading band: B01\n",
      "    Band B01 shape: (1, 1830, 1830), chunks: ((1,), (1830,), (1830,))\n",
      "  Loading band: B02\n",
      "    Band B02 shape: (1, 10980, 10980), chunks: ((1,), (2048, 2048, 2048, 2048, 2048, 740), (2048, 2048, 2048, 2048, 2048, 740))\n",
      "  Loading band: B03\n",
      "    Band B03 shape: (1, 10980, 10980), chunks: ((1,), (2048, 2048, 2048, 2048, 2048, 740), (2048, 2048, 2048, 2048, 2048, 740))\n",
      "  Loading band: B04\n",
      "    Band B04 shape: (1, 10980, 10980), chunks: ((1,), (2048, 2048, 2048, 2048, 2048, 740), (2048, 2048, 2048, 2048, 2048, 740))\n",
      "  Loading band: B05\n",
      "    Band B05 shape: (1, 5490, 5490), chunks: ((1,), (2048, 2048, 1394), (2048, 2048, 1394))\n",
      "  Loading band: B06\n",
      "    Band B06 shape: (1, 5490, 5490), chunks: ((1,), (2048, 2048, 1394), (2048, 2048, 1394))\n",
      "  Loading band: B07\n",
      "    Band B07 shape: (1, 5490, 5490), chunks: ((1,), (2048, 2048, 1394), (2048, 2048, 1394))\n",
      "  Loading band: B08\n",
      "    Band B08 shape: (1, 10980, 10980), chunks: ((1,), (2048, 2048, 2048, 2048, 2048, 740), (2048, 2048, 2048, 2048, 2048, 740))\n",
      "  Loading band: B09\n",
      "    Band B09 shape: (1, 1830, 1830), chunks: ((1,), (1830,), (1830,))\n",
      "  Loading band: B11\n",
      "    Band B11 shape: (1, 5490, 5490), chunks: ((1,), (2048, 2048, 1394), (2048, 2048, 1394))\n",
      "  Loading band: B12\n",
      "    Band B12 shape: (1, 5490, 5490), chunks: ((1,), (2048, 2048, 1394), (2048, 2048, 1394))\n",
      "  Loading band: B8A\n",
      "    Band B8A shape: (1, 5490, 5490), chunks: ((1,), (2048, 2048, 1394), (2048, 2048, 1394))\n",
      "Concatenating bands by resolution groups...\n",
      "  Concatenating 4 bands at 10m resolution\n",
      "  Concatenating 6 bands at 20m resolution\n",
      "  Concatenating 2 bands at 60m resolution\n",
      "Using 10m resolution bands for output\n",
      "Multi-band array shape: (4, 10980, 10980)\n",
      "Multi-band chunks: ((1, 1, 1, 1), (2048, 2048, 2048, 2048, 2048, 740), (2048, 2048, 2048, 2048, 2048, 740))\n",
      "Estimated memory per chunk: 6697800.00 bytes\n",
      "Saving to: /home/nissim/Documents/dev/arg-inundaciones/data/s2_row_25_allbands_20160113.tif\n",
      "Starting chunked write to COG (this may take a while)...\n",
      "Saved multi-band COG successfully!\n",
      "File size: 1197.01 MB\n"
     ]
    }
   ],
   "source": [
    "import rioxarray as rio\n",
    "import os\n",
    "import xarray as xr\n",
    "import dask\n",
    "from dask.distributed import Client\n",
    "\n",
    "# Configure Dask for memory-efficient processing\n",
    "# Limit concurrent tasks to control peak memory usage\n",
    "dask.config.set(\n",
    "    {\n",
    "        \"array.slicing.split_large_chunks\": False,\n",
    "        \"distributed.worker.memory.target\": 0.8,  # Use 80% of worker memory before spilling\n",
    "        \"distributed.worker.memory.spill\": 0.9,  # Spill to disk at 90%\n",
    "    }\n",
    ")\n",
    "\n",
    "# Optional: Start a local Dask client with limited workers\n",
    "# Adjust n_workers based on your system (start with 2-4 for 64GB RAM)\n",
    "client = Client(n_workers=4, threads_per_worker=2, memory_limit=\"8GB\")\n",
    "print(f\"Dask dashboard: {client.dashboard_link}\")\n",
    "\n",
    "# Get the first row with imagery from your S2 results\n",
    "rows_with_imagery = results_df[results_df[\"items_found\"] > 0]\n",
    "if len(rows_with_imagery) > 0:\n",
    "    # Get the first row with imagery\n",
    "    first_row = rows_with_imagery.iloc[0]\n",
    "    row_index = first_row[\"row_index\"]\n",
    "    print(\n",
    "        f\"Loading Sentinel-2 imagery for Row {row_index}: {first_row['items_found']} items\"\n",
    "    )\n",
    "\n",
    "    # Use .loc to get the correct row\n",
    "    original_row = emdat_filtered.loc[row_index]\n",
    "    bbox_geometry, bbox_dict = get_flood_bounding_box(original_row, \"ARG\")\n",
    "    bounds = bbox_geometry.bounds\n",
    "    bbox = [bounds[0], bounds[1], bounds[2], bounds[3]]\n",
    "    datetime_range = original_row.get(\"datetime_range\")\n",
    "\n",
    "    # Search for Sentinel-2 L2A data\n",
    "    search = catalog.search(\n",
    "        collections=[\"sentinel-2-l2a\"],\n",
    "        bbox=bbox,\n",
    "        datetime=datetime_range,\n",
    "        query={\n",
    "            \"eo:cloud_cover\": {\"lt\": 20},  # Less than 20% cloud cover\n",
    "        },\n",
    "    )\n",
    "    items = search.item_collection()\n",
    "\n",
    "    # Load all bands for the first item with Dask chunking\n",
    "    item = items[0]\n",
    "    print(f\"Loading item: {item.id}\")\n",
    "    print(f\"Date: {item.datetime}\")\n",
    "\n",
    "    # List all band asset keys (usually B01, B02, ..., B12, B8A, etc.)\n",
    "    band_keys = [k for k in item.assets.keys() if k.startswith(\"B\")]\n",
    "    band_keys.sort()  # Sort for consistency\n",
    "    print(f\"Bands to load: {band_keys}\")\n",
    "\n",
    "    # Define chunk size for memory-efficient processing\n",
    "    # 2048x2048 is usually good for regional data - adjust if needed\n",
    "    chunk_size = {\"x\": 2048, \"y\": 2048}\n",
    "\n",
    "    # Group bands by resolution to avoid unwanted resampling during concatenation\n",
    "    bands_10m = []  # B02, B03, B04, B08 (10980x10980)\n",
    "    bands_20m = []  # B05, B06, B07, B11, B12, B8A (5490x5490)\n",
    "    bands_60m = []  # B01, B09 (1830x1830)\n",
    "\n",
    "    print(\"Loading bands with Dask chunking...\")\n",
    "    for band in band_keys:\n",
    "        print(f\"  Loading band: {band}\")\n",
    "        # Load with chunking - this creates a Dask array (lazy evaluation)\n",
    "        da = rio.open_rasterio(\n",
    "            item.assets[band].href,\n",
    "            chunks=chunk_size,\n",
    "            lock=False,  # Allow concurrent access to the same file\n",
    "        )\n",
    "        # Set the band name for clarity\n",
    "        da = da.assign_coords(band=[band])\n",
    "\n",
    "        # Group by resolution based on spatial dimensions\n",
    "        if da.shape[1] > 8000:  # 10m bands (~10980x10980)\n",
    "            bands_10m.append(da)\n",
    "        elif da.shape[1] > 4000:  # 20m bands (~5490x5490)\n",
    "            bands_20m.append(da)\n",
    "        else:  # 60m bands (~1830x1830)\n",
    "            bands_60m.append(da)\n",
    "\n",
    "        print(f\"    Band {band} shape: {da.shape}, chunks: {da.chunks}\")\n",
    "\n",
    "    print(\"Concatenating bands by resolution groups...\")\n",
    "\n",
    "    # Process each resolution group separately to maintain chunking\n",
    "    resolution_groups = []\n",
    "\n",
    "    if bands_10m:\n",
    "        print(f\"  Concatenating {len(bands_10m)} bands at 10m resolution\")\n",
    "        multi_10m = xr.concat(bands_10m, dim=\"band\")\n",
    "        resolution_groups.append((\"10m\", multi_10m))\n",
    "\n",
    "    if bands_20m:\n",
    "        print(f\"  Concatenating {len(bands_20m)} bands at 20m resolution\")\n",
    "        multi_20m = xr.concat(bands_20m, dim=\"band\")\n",
    "        resolution_groups.append((\"20m\", multi_20m))\n",
    "\n",
    "    if bands_60m:\n",
    "        print(f\"  Concatenating {len(bands_60m)} bands at 60m resolution\")\n",
    "        multi_60m = xr.concat(bands_60m, dim=\"band\")\n",
    "        resolution_groups.append((\"60m\", multi_60m))\n",
    "\n",
    "    # For now, let's work with the highest resolution group (10m)\n",
    "    # You can modify this logic based on your needs\n",
    "    if bands_10m:\n",
    "        multi_band = multi_10m\n",
    "        print(\"Using 10m resolution bands for output\")\n",
    "    elif bands_20m:\n",
    "        multi_band = multi_20m\n",
    "        print(\"Using 20m resolution bands for output\")\n",
    "    else:\n",
    "        multi_band = multi_60m\n",
    "        print(\"Using 60m resolution bands for output\")\n",
    "\n",
    "    print(f\"Multi-band array shape: {multi_band.shape}\")\n",
    "    print(f\"Multi-band chunks: {multi_band.chunks}\")\n",
    "    # Calculate estimated memory per chunk (chunks is a tuple of tuples)\n",
    "    chunk_sizes = [len(c) for c in multi_band.chunks]\n",
    "    total_chunks = np.prod(chunk_sizes)\n",
    "    print(f\"Estimated memory per chunk: {multi_band.nbytes / total_chunks:.2f} bytes\")\n",
    "\n",
    "    # Create output directory if it doesn't exist\n",
    "    output_dir = \"/home/nissim/Documents/dev/arg-inundaciones/data/\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Create filename\n",
    "    filename = f\"s2_row_{row_index}_allbands_{item.datetime.strftime('%Y%m%d')}.tif\"\n",
    "    output_path = os.path.join(output_dir, filename)\n",
    "    print(f\"Saving to: {output_path}\")\n",
    "\n",
    "    # Save as COG with Dask-enabled chunked writing\n",
    "    # This will process chunks sequentially, keeping memory usage bounded\n",
    "    print(\"Starting chunked write to COG (this may take a while)...\")\n",
    "    with dask.config.set(scheduler=\"threads\"):  # Use threads for I/O\n",
    "        multi_band.rio.to_raster(\n",
    "            output_path,\n",
    "            driver=\"COG\",\n",
    "            compress=\"LZW\",\n",
    "            tiled=True,\n",
    "            blockxsize=512,\n",
    "            blockysize=512,\n",
    "            overview_levels=[2, 4, 8, 16],\n",
    "            overview_resampling=\"nearest\",\n",
    "            # COG-specific optimizations\n",
    "            bigtiff=\"auto\",  # Use BigTIFF for large files\n",
    "        )\n",
    "\n",
    "    print(\"Saved multi-band COG successfully!\")\n",
    "    print(f\"File size: {os.path.getsize(output_path) / (1024 * 1024):.2f} MB\")\n",
    "\n",
    "    # Clean up Dask client\n",
    "    client.close()\n",
    "\n",
    "else:\n",
    "    print(\"No Sentinel-2 imagery found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Band coordinates: [1 2 3 4]\n"
     ]
    }
   ],
   "source": [
    "import rioxarray as rio\n",
    "\n",
    "da = rio.open_rasterio(\n",
    "    \"/home/nissim/Documents/dev/arg-inundaciones/data/s2_row_25_allbands_20160113.tif\"\n",
    ")\n",
    "print(f\"Band coordinates: {da.coords['band'].values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# so, we will have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so, we'll have:\n",
    "# 1: download raw imagery\n",
    "# 2: process raw imagery compared to pre-flood imagery\n",
    "# 3: apply algorithmic corrections\n",
    "# 4: manually correct remaining imagery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
